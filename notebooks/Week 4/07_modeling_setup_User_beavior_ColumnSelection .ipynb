{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a784309-d671-41ce-88bc-b71dcbda4517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD:\n",
      "\n",
      "                                           msno win_bucket  num_25  num_50  \\\n",
      "0  7TsvwHeE6/cqQTJgpIHpoAgdZ9rWThUmNQTt813pa5Q=       long      34      11   \n",
      "1  y4lnXrZR0J+C7CWKrC9HIZ3PDbpyh5LmmQ/8GWw2DhQ=        mid     185      50   \n",
      "2  4NXcz8rgeuUH/78LlSQvk/D1i55tVwk16QwqKs6yVEQ=       long       6       3   \n",
      "3  8IggSrncutI6yYxrAlHITwI/UY6FJRq/EeaIFZUkhME=     recent      13       4   \n",
      "4  r4g94WaFbTiGGo7xwyuJbDvmluZjrCVym94v84mrhfk=     recent       2       0   \n",
      "\n",
      "   num_75  num_985  num_100  num_unq  total_secs  \n",
      "0       2        4      216      142   54910.227  \n",
      "1      35       40      273      481   79740.885  \n",
      "2       1        0       32       40    7786.473  \n",
      "3       5        5      314      309   78091.763  \n",
      "4       0        1      189      190   42333.155  \n",
      "\n",
      "DESCRIBE:\n",
      "\n",
      "             num_25        num_50        num_75       num_985       num_100  \\\n",
      "count  3.205439e+06  3.205439e+06  3.205439e+06  3.205439e+06  3.205439e+06   \n",
      "mean   9.778616e+01  2.462121e+01  1.521563e+01  1.670627e+01  4.541590e+02   \n",
      "std    1.783438e+02  3.949653e+01  2.250405e+01  3.501202e+01  7.178619e+02   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.100000e+01  3.000000e+00  2.000000e+00  2.000000e+00  4.400000e+01   \n",
      "50%    4.300000e+01  1.200000e+01  8.000000e+00  8.000000e+00  2.200000e+02   \n",
      "75%    1.180000e+02  3.100000e+01  2.000000e+01  2.100000e+01  5.680000e+02   \n",
      "max    1.078750e+05  5.474000e+03  2.035000e+03  4.997000e+03  6.033900e+04   \n",
      "\n",
      "            num_unq    total_secs  \n",
      "count  3.205439e+06  3.205439e+06  \n",
      "mean   4.484593e+02 -3.078831e+11  \n",
      "std    5.891243e+02  7.339902e+13  \n",
      "min    1.000000e+00 -9.223372e+16  \n",
      "25%    6.000000e+01  1.339026e+04  \n",
      "50%    2.510000e+02  6.128946e+04  \n",
      "75%    6.090000e+02  1.539158e+05  \n",
      "max    2.412000e+04  1.457028e+09  \n",
      "\n",
      "BUCKET-VALUES:\n",
      "\n",
      "win_bucket\n",
      "recent    1091889\n",
      "mid       1067073\n",
      "long      1046477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "print(f\"HEAD:\\n\\n{agg_logs.head()}\\n\\nDESCRIBE:\\n\\n{agg_logs.describe()}\\n\\nBUCKET-VALUES:\\n\\n{agg_logs[\"win_bucket\"].value_counts()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79775a42-a052-4836-9dbd-5fa8056dd71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD:\n",
      "\n",
      "                                           msno  long_num_25  mid_num_25  \\\n",
      "0  +++IZseRRiQS9aaSkH6cMYU6bGDcxUieAi/tH67sC5s=         40.0        35.0   \n",
      "1  +++dz9ZCWE2HB/47pJU82NJXQzQuZDx1Wm50YSk/kKk=          5.0         0.0   \n",
      "2  +++l/EXNMLTijfLBa8p2TUVVVp2aFGSuUI/h7mLmthw=         63.0        60.0   \n",
      "3  +++snpr7pmobhLKUgSHTv/mpkqgBT0tQJ0zQj6qKrqc=        164.0       178.0   \n",
      "4  ++/9R3sX37CjxbY/AaGvbwr3QkwElKBCtSvVzhCBDOk=         39.0       118.0   \n",
      "\n",
      "   recent_num_25  long_num_50  mid_num_50  recent_num_50  long_num_75  \\\n",
      "0           61.0         13.0        18.0           13.0          8.0   \n",
      "1            0.0          1.0         0.0            0.0          1.0   \n",
      "2          101.0         19.0        22.0           28.0         16.0   \n",
      "3          128.0         49.0        52.0           50.0         32.0   \n",
      "4           67.0          6.0        36.0           37.0         13.0   \n",
      "\n",
      "   mid_num_75  recent_num_75  ...  recent_num_985  long_num_100  mid_num_100  \\\n",
      "0         9.0           10.0  ...            15.0        3069.0       2958.0   \n",
      "1         0.0            0.0  ...             0.0         183.0          0.0   \n",
      "2        21.0           35.0  ...            23.0         579.0        622.0   \n",
      "3        22.0           48.0  ...            15.0         561.0        473.0   \n",
      "4        24.0           19.0  ...            25.0         253.0        397.0   \n",
      "\n",
      "   recent_num_100  long_num_unq  mid_num_unq  recent_num_unq  long_total_secs  \\\n",
      "0           312.0        3112.0       2703.0           396.0       748650.942   \n",
      "1             0.0         147.0          0.0             0.0        46731.474   \n",
      "2           661.0         621.0        545.0           766.0       145294.028   \n",
      "3           139.0         682.0        677.0           306.0       155627.747   \n",
      "4           120.0          80.0        306.0           144.0        69861.777   \n",
      "\n",
      "   mid_total_secs  recent_total_secs  \n",
      "0      749799.381          86148.514  \n",
      "1           0.000              0.000  \n",
      "2      157429.600         172602.177  \n",
      "3      139214.915          52634.967  \n",
      "4      113404.582          43677.946  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "DESCRIBE:\n",
      "\n",
      "        long_num_25    mid_num_25  recent_num_25   long_num_50    mid_num_50  \\\n",
      "count  1.576331e+06  1.576331e+06   1.576331e+06  1.576331e+06  1.576331e+06   \n",
      "mean   6.751574e+01  6.601493e+01   6.531561e+01  1.649459e+01  1.678013e+01   \n",
      "std    1.737317e+02  1.423096e+02   1.431963e+02  3.527600e+01  3.382211e+01   \n",
      "min    0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    1.100000e+01  1.300000e+01   1.400000e+01  3.000000e+00  3.000000e+00   \n",
      "75%    7.200000e+01  7.400000e+01   7.300000e+01  1.900000e+01  2.000000e+01   \n",
      "max    1.078750e+05  2.873200e+04   4.331000e+04  5.101000e+03  4.436000e+03   \n",
      "\n",
      "       recent_num_50   long_num_75    mid_num_75  recent_num_75  long_num_985  \\\n",
      "count   1.576331e+06  1.576331e+06  1.576331e+06   1.576331e+06  1.576331e+06   \n",
      "mean    1.679204e+01  9.822669e+00  1.061616e+01   1.050187e+01  1.039396e+01   \n",
      "std     3.436479e+01  1.951156e+01  2.010161e+01   1.990857e+01  2.600616e+01   \n",
      "min     0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00   \n",
      "25%     0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00   \n",
      "50%     4.000000e+00  2.000000e+00  2.000000e+00   2.000000e+00  2.000000e+00   \n",
      "75%     2.000000e+01  1.200000e+01  1.400000e+01   1.300000e+01  1.200000e+01   \n",
      "max     5.474000e+03  1.521000e+03  1.416000e+03   2.035000e+03  4.440000e+03   \n",
      "\n",
      "       ...  recent_num_985  long_num_100   mid_num_100  recent_num_100  \\\n",
      "count  ...    1.576331e+06  1.576331e+06  1.576331e+06    1.576331e+06   \n",
      "mean   ...    1.166432e+01  3.034635e+02  3.106716e+02    3.093885e+02   \n",
      "std    ...    3.147563e+01  6.312344e+02  6.292811e+02    6.233395e+02   \n",
      "min    ...    0.000000e+00  0.000000e+00  0.000000e+00    0.000000e+00   \n",
      "25%    ...    0.000000e+00  0.000000e+00  0.000000e+00    0.000000e+00   \n",
      "50%    ...    2.000000e+00  4.200000e+01  5.100000e+01    5.800000e+01   \n",
      "75%    ...    1.400000e+01  3.590000e+02  3.760000e+02    3.700000e+02   \n",
      "max    ...    4.979000e+03  6.033900e+04  5.509100e+04    4.782900e+04   \n",
      "\n",
      "       long_num_unq   mid_num_unq  recent_num_unq  long_total_secs  \\\n",
      "count  1.576331e+06  1.576331e+06    1.576331e+06     1.576331e+06   \n",
      "mean   3.022132e+02  3.064548e+02    3.032654e+02    -6.260745e+11   \n",
      "std    5.325154e+02  5.298413e+02    5.226841e+02     1.046662e+14   \n",
      "min    0.000000e+00  0.000000e+00    0.000000e+00    -9.223372e+16   \n",
      "25%    0.000000e+00  0.000000e+00    0.000000e+00     0.000000e+00   \n",
      "50%    5.700000e+01  6.600000e+01    7.500000e+01     1.248954e+04   \n",
      "75%    4.030000e+02  4.140000e+02    4.050000e+02     9.802365e+04   \n",
      "max    1.599800e+04  2.412000e+04    2.108300e+04     1.457028e+09   \n",
      "\n",
      "       mid_total_secs  recent_total_secs  \n",
      "count    1.576331e+06       1.576331e+06  \n",
      "mean     8.224579e+04       8.201886e+04  \n",
      "std      1.560308e+05       1.558313e+05  \n",
      "min      0.000000e+00       0.000000e+00  \n",
      "25%      0.000000e+00       0.000000e+00  \n",
      "50%      1.523974e+04       1.722117e+04  \n",
      "75%      1.033097e+05       1.016958e+05  \n",
      "max      1.301664e+07       1.138590e+07  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "print(f\"HEAD:\\n\\n{pivoted.head()}\\n\\nDESCRIBE:\\n\\n{pivoted.describe()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0371d8-2b6f-488d-bb9f-d2c5ad0a1fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD:\n",
      "\n",
      "                                           msno  is_churn  total_amount_paid  \\\n",
      "0  waLDQMmcOu2jLDaV1ddDkgCrB/jl6sD66Xzs0Vqax1Y=         1                0.0   \n",
      "1  QA7uiXy8vIbUSPOkCf9RwQ3FsT8jVq2OxDr8zqa7bRQ=         1              298.0   \n",
      "2  fGwBva6hikQmTJzrbz/2Ezjm5Cth5jZUNvXigKK2AFA=         1              149.0   \n",
      "3  mT5V8rEpa+8wuqi6x0DoVd3H5icMKkE9Prt49UlmK+4=         1                0.0   \n",
      "4  XaPhtGLk/5UvvOYHcONTwsnH97P4eGECeq+BARGItRw=         1                0.0   \n",
      "\n",
      "   num_transactions  num_cancellations  total_plan_days  long_num_25  \\\n",
      "0               0.0                0.0              0.0          0.0   \n",
      "1               2.0                0.0             60.0          2.0   \n",
      "2               1.0                0.0             30.0         93.0   \n",
      "3               0.0                0.0              0.0        200.0   \n",
      "4               0.0                0.0              0.0         63.0   \n",
      "\n",
      "   mid_num_25  recent_num_25  long_num_50  ...  recent_num_985  long_num_100  \\\n",
      "0         0.0            0.0          0.0  ...             0.0           0.0   \n",
      "1        14.0           12.0          0.0  ...             6.0          31.0   \n",
      "2       141.0          121.0         67.0  ...            43.0        1969.0   \n",
      "3       408.0          385.0         86.0  ...            49.0         860.0   \n",
      "4       149.0           48.0         44.0  ...            37.0        3665.0   \n",
      "\n",
      "   mid_num_100  recent_num_100  long_num_unq  mid_num_unq  recent_num_unq  \\\n",
      "0          0.0             0.0           0.0          0.0             0.0   \n",
      "1        220.0           180.0          22.0        123.0           102.0   \n",
      "2       1439.0          1055.0        1902.0       1596.0          1334.0   \n",
      "3        884.0          1265.0         768.0        886.0          1403.0   \n",
      "4       3368.0          2397.0        3811.0       3564.0          2468.0   \n",
      "\n",
      "   long_total_secs  mid_total_secs  recent_total_secs  \n",
      "0            0.000           0.000              0.000  \n",
      "1         8304.385       61003.350          51031.900  \n",
      "2       476142.077      358898.544         281293.581  \n",
      "3       227874.951      273152.183         312991.990  \n",
      "4       961418.681      880715.218         629925.240  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "DESCRIBE:\n",
      "\n",
      "            is_churn  total_amount_paid  num_transactions  num_cancellations  \\\n",
      "count  992931.000000      992931.000000     992931.000000      992931.000000   \n",
      "mean        0.063923         197.895464          1.501581           0.019201   \n",
      "std         0.244616         127.079511          0.839490           0.139732   \n",
      "min         0.000000           0.000000          0.000000           0.000000   \n",
      "25%         0.000000         149.000000          1.000000           0.000000   \n",
      "50%         0.000000         198.000000          2.000000           0.000000   \n",
      "75%         0.000000         298.000000          2.000000           0.000000   \n",
      "max         1.000000        1620.000000         37.000000           4.000000   \n",
      "\n",
      "       total_plan_days    long_num_25     mid_num_25  recent_num_25  \\\n",
      "count    992931.000000  992931.000000  992931.000000  992931.000000   \n",
      "mean         46.207315      66.959494      69.014569      70.487062   \n",
      "std          28.024033     187.204741     148.059995     150.854947   \n",
      "min           0.000000       0.000000       0.000000       0.000000   \n",
      "25%          30.000000       0.000000       0.000000       0.000000   \n",
      "50%          60.000000       7.000000      14.000000      18.000000   \n",
      "75%          60.000000      72.000000      79.000000      82.000000   \n",
      "max         500.000000  107875.000000   28732.000000   43310.000000   \n",
      "\n",
      "         long_num_50     mid_num_50  ...  recent_num_985   long_num_100  \\\n",
      "count  992931.000000  992931.000000  ...   992931.000000  992931.000000   \n",
      "mean       16.094569      17.335483  ...       12.600962     302.294967   \n",
      "std        35.309120      34.419918  ...       31.812042     618.338211   \n",
      "min         0.000000       0.000000  ...        0.000000       0.000000   \n",
      "25%         0.000000       0.000000  ...        0.000000       0.000000   \n",
      "50%         2.000000       4.000000  ...        3.000000      35.000000   \n",
      "75%        19.000000      21.000000  ...       15.000000     363.000000   \n",
      "max      5101.000000    4436.000000  ...     4979.000000   41603.000000   \n",
      "\n",
      "         mid_num_100  recent_num_100   long_num_unq    mid_num_unq  \\\n",
      "count  992931.000000   992931.000000  992931.000000  992931.000000   \n",
      "mean      324.152295      332.254858     300.550553     320.909223   \n",
      "std       626.564620      628.150592     531.499166     535.593941   \n",
      "min         0.000000        0.000000       0.000000       0.000000   \n",
      "25%         0.000000        0.000000       0.000000       0.000000   \n",
      "50%        70.000000       92.000000      48.000000      89.000000   \n",
      "75%       402.000000      410.000000     406.000000     443.000000   \n",
      "max     55091.000000    47829.000000   15998.000000   24120.000000   \n",
      "\n",
      "       recent_num_unq  long_total_secs  mid_total_secs  recent_total_secs  \n",
      "count   992931.000000     9.929310e+05    9.929310e+05       9.929310e+05  \n",
      "mean       328.648662    -5.108969e+11    8.588757e+04       8.810493e+04  \n",
      "std        532.586469     7.109601e+13    1.573579e+05       1.577488e+05  \n",
      "min          0.000000    -1.844674e+16    0.000000e+00       0.000000e+00  \n",
      "25%          0.000000     0.000000e+00    0.000000e+00       0.000000e+00  \n",
      "50%        113.000000     1.059606e+04    2.053568e+04       2.678454e+04  \n",
      "75%        451.000000     9.895971e+04    1.101853e+05       1.124249e+05  \n",
      "max      21083.000000     7.135216e+07    1.301664e+07       1.138590e+07  \n",
      "\n",
      "[8 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "print(f\"HEAD:\\n\\n{data.head()}\\n\\nDESCRIBE:\\n\\n{data.describe()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d88c2e4-d751-482b-b3ec-1946ea38d754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>median_churn</th>\n",
       "      <th>median_nonchurn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>total_amount_paid</td>\n",
       "      <td>149.000</td>\n",
       "      <td>198.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num_transactions</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_cancellations</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_plan_days</td>\n",
       "      <td>30.000</td>\n",
       "      <td>60.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>long_num_25</td>\n",
       "      <td>2.000</td>\n",
       "      <td>7.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mid_num_25</td>\n",
       "      <td>4.000</td>\n",
       "      <td>14.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>recent_num_25</td>\n",
       "      <td>8.000</td>\n",
       "      <td>18.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long_num_50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mid_num_50</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>recent_num_50</td>\n",
       "      <td>2.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>long_num_75</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mid_num_75</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>recent_num_75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>long_num_985</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mid_num_985</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>recent_num_985</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>long_num_100</td>\n",
       "      <td>5.000</td>\n",
       "      <td>37.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mid_num_100</td>\n",
       "      <td>17.000</td>\n",
       "      <td>74.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recent_num_100</td>\n",
       "      <td>39.000</td>\n",
       "      <td>96.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>long_num_unq</td>\n",
       "      <td>12.000</td>\n",
       "      <td>51.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mid_num_unq</td>\n",
       "      <td>26.000</td>\n",
       "      <td>93.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>recent_num_unq</td>\n",
       "      <td>53.000</td>\n",
       "      <td>117.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>long_total_secs</td>\n",
       "      <td>2084.250</td>\n",
       "      <td>11220.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mid_total_secs</td>\n",
       "      <td>5491.811</td>\n",
       "      <td>21523.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>recent_total_secs</td>\n",
       "      <td>11871.442</td>\n",
       "      <td>27761.014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  median_churn  median_nonchurn\n",
       "0   total_amount_paid       149.000          198.000\n",
       "1    num_transactions         1.000            2.000\n",
       "2   num_cancellations         0.000            0.000\n",
       "3     total_plan_days        30.000           60.000\n",
       "4         long_num_25         2.000            7.000\n",
       "5          mid_num_25         4.000           14.000\n",
       "6       recent_num_25         8.000           18.000\n",
       "7         long_num_50         0.000            2.000\n",
       "8          mid_num_50         1.000            4.000\n",
       "9       recent_num_50         2.000            5.000\n",
       "10        long_num_75         0.000            1.000\n",
       "11         mid_num_75         0.000            3.000\n",
       "12      recent_num_75         1.000            3.000\n",
       "13       long_num_985         0.000            1.000\n",
       "14        mid_num_985         0.000            3.000\n",
       "15     recent_num_985         1.000            3.000\n",
       "16       long_num_100         5.000           37.000\n",
       "17        mid_num_100        17.000           74.000\n",
       "18     recent_num_100        39.000           96.000\n",
       "19       long_num_unq        12.000           51.000\n",
       "20        mid_num_unq        26.000           93.000\n",
       "21     recent_num_unq        53.000          117.000\n",
       "22    long_total_secs      2084.250        11220.185\n",
       "23     mid_total_secs      5491.811        21523.988\n",
       "24  recent_total_secs     11871.442        27761.014"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "signal_results = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in [\"msno\", \"is_churn\"]:\n",
    "        continue\n",
    "\n",
    "    med = data.groupby(\"is_churn\")[col].median()\n",
    "\n",
    "    signal_results.append({\n",
    "        \"feature\": col,\n",
    "        \"median_churn\": med.loc[1],\n",
    "        \"median_nonchurn\": med.loc[0]\n",
    "    })\n",
    "\n",
    "signal_df = pd.DataFrame(signal_results)\n",
    "signal_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6757367-d9c3-4670-8546-452cbd233795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_25', 'num_50', 'num_75', 'num_985', 'num_100', 'num_unq', 'total_secs']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "signal_results = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in [\"msno\", \"is_churn\"]:\n",
    "        continue\n",
    "\n",
    "    med = data.groupby(\"is_churn\")[col].median()\n",
    "\n",
    "    signal_results.append({\n",
    "        \"feature\": col,\n",
    "        \"median_churn\": med.loc[1],\n",
    "        \"median_nonchurn\": med.loc[0]\n",
    "    })\n",
    "\n",
    "signal_df = pd.DataFrame(signal_results)\n",
    "\n",
    "\n",
    "signal_df[\"base_col\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "signal_df[\"window\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "\n",
    "behavior_signal_df = signal_df[\n",
    "    signal_df[\"feature\"].str.contains(\"_\", regex=False)\n",
    "].copy()\n",
    "\n",
    "\n",
    "behavior_signal_df[\"window\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "behavior_signal_df[\"base_col\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "\n",
    "\n",
    "robust_cols = []\n",
    "\n",
    "for base in behavior_signal_df[\"base_col\"].unique():\n",
    "    temp = behavior_signal_df[\n",
    "        behavior_signal_df[\"base_col\"] == base\n",
    "    ]\n",
    "\n",
    "    strong_windows = 0\n",
    "    for _, row in temp.iterrows():\n",
    "        if row[\"median_nonchurn\"] > row[\"median_churn\"]:\n",
    "            strong_windows += 1\n",
    "\n",
    "    if strong_windows >= 2:\n",
    "        robust_cols.append(base)\n",
    "\n",
    "robust_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552e48d1-15b1-4637-bb69-1fc5e6504e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "signal_results = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in [\"msno\", \"is_churn\"]:\n",
    "        continue\n",
    "\n",
    "    med = data.groupby(\"is_churn\")[col].median()\n",
    "\n",
    "    signal_results.append({\n",
    "        \"feature\": col,\n",
    "        \"median_churn\": med.loc[1],\n",
    "        \"median_nonchurn\": med.loc[0]\n",
    "    })\n",
    "\n",
    "signal_df = pd.DataFrame(signal_results)\n",
    "\n",
    "\n",
    "signal_df[\"base_col\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "signal_df[\"window\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "\n",
    "behavior_signal_df = signal_df[\n",
    "    signal_df[\"feature\"].str.contains(\"_\", regex=False)\n",
    "].copy()\n",
    "\n",
    "\n",
    "behavior_signal_df[\"window\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "behavior_signal_df[\"base_col\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "\n",
    "\n",
    "robust_cols = []\n",
    "\n",
    "for base in behavior_signal_df[\"base_col\"].unique():\n",
    "    temp = behavior_signal_df[\n",
    "        behavior_signal_df[\"base_col\"] == base\n",
    "    ]\n",
    "\n",
    "    strong_windows = 0\n",
    "    for _, row in temp.iterrows():\n",
    "        if row[\"median_nonchurn\"] > row[\"median_churn\"]:\n",
    "            strong_windows += 1\n",
    "\n",
    "    if strong_windows >= 2:\n",
    "        robust_cols.append(base)\n",
    "\n",
    "candidate_features = []\n",
    "\n",
    "for base in robust_cols:\n",
    "    for w in [\"recent\", \"mid\", \"long\"]:\n",
    "        candidate_features.append(f\"{w}_{base}\")\n",
    "\n",
    "corr = data[candidate_features].corr().abs()\n",
    "corr.to_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/coorelation_result.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4faa93-6bdd-4be6-9295-903c7d33a962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6928714599749448"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "signal_results = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in [\"msno\", \"is_churn\"]:\n",
    "        continue\n",
    "\n",
    "    med = data.groupby(\"is_churn\")[col].median()\n",
    "\n",
    "    signal_results.append({\n",
    "        \"feature\": col,\n",
    "        \"median_churn\": med.loc[1],\n",
    "        \"median_nonchurn\": med.loc[0]\n",
    "    })\n",
    "\n",
    "signal_df = pd.DataFrame(signal_results)\n",
    "\n",
    "\n",
    "signal_df[\"base_col\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "signal_df[\"window\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "\n",
    "behavior_signal_df = signal_df[\n",
    "    signal_df[\"feature\"].str.contains(\"_\", regex=False)\n",
    "].copy()\n",
    "\n",
    "\n",
    "behavior_signal_df[\"window\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "behavior_signal_df[\"base_col\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "\n",
    "\n",
    "robust_cols = []\n",
    "\n",
    "for base in behavior_signal_df[\"base_col\"].unique():\n",
    "    temp = behavior_signal_df[\n",
    "        behavior_signal_df[\"base_col\"] == base\n",
    "    ]\n",
    "\n",
    "    strong_windows = 0\n",
    "    for _, row in temp.iterrows():\n",
    "        if row[\"median_nonchurn\"] > row[\"median_churn\"]:\n",
    "            strong_windows += 1\n",
    "\n",
    "    if strong_windows >= 2:\n",
    "        robust_cols.append(base)\n",
    "\n",
    "candidate_features = []\n",
    "\n",
    "for base in robust_cols:\n",
    "    for w in [\"recent\", \"mid\", \"long\"]:\n",
    "        candidate_features.append(f\"{w}_{base}\")\n",
    "\n",
    "\n",
    "final_behavior_features = [\n",
    "    \"recent_num_25\",\n",
    "    \"recent_num_50\",\n",
    "    \"recent_num_985\",\n",
    "    \"mid_num_25\",\n",
    "    \"mid_num_50\",\n",
    "    \"mid_num_985\",\n",
    "    \"mid_num_unq\",\n",
    "    \"long_num_25\",\n",
    "    \"long_num_50\",\n",
    "    \"long_num_985\",\n",
    "    \"long_num_unq\",\n",
    "    \"long_total_secs\"\n",
    "]\n",
    "\n",
    "\n",
    "# Week 3 Base ROC Calculation \n",
    "X_base = baseline.drop(columns=[\"msno\", \"is_churn\"])\n",
    "y = baseline[\"is_churn\"]\n",
    "\n",
    "Xb_tr, Xb_val, y_tr, y_val = train_test_split(\n",
    "    X_base, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_base = LogisticRegression(max_iter=1000)\n",
    "model_base.fit(Xb_tr, y_tr)\n",
    "\n",
    "roc_base = roc_auc_score(\n",
    "    y_val,\n",
    "    model_base.predict_proba(Xb_val)[:, 1]\n",
    ")\n",
    "\n",
    "roc_base\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ecc84c-6b14-484e-821d-9f892c2ea001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 3 ROC:0.6928714599749448 & Hopefully Improved Week 4 ROC:0.6985777033999063\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "signal_results = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in [\"msno\", \"is_churn\"]:\n",
    "        continue\n",
    "\n",
    "    med = data.groupby(\"is_churn\")[col].median()\n",
    "\n",
    "    signal_results.append({\n",
    "        \"feature\": col,\n",
    "        \"median_churn\": med.loc[1],\n",
    "        \"median_nonchurn\": med.loc[0]\n",
    "    })\n",
    "\n",
    "signal_df = pd.DataFrame(signal_results)\n",
    "\n",
    "\n",
    "signal_df[\"base_col\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "signal_df[\"window\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "\n",
    "behavior_signal_df = signal_df[\n",
    "    signal_df[\"feature\"].str.contains(\"_\", regex=False)\n",
    "].copy()\n",
    "\n",
    "\n",
    "behavior_signal_df[\"window\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "behavior_signal_df[\"base_col\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "\n",
    "\n",
    "robust_cols = []\n",
    "\n",
    "for base in behavior_signal_df[\"base_col\"].unique():\n",
    "    temp = behavior_signal_df[\n",
    "        behavior_signal_df[\"base_col\"] == base\n",
    "    ]\n",
    "\n",
    "    strong_windows = 0\n",
    "    for _, row in temp.iterrows():\n",
    "        if row[\"median_nonchurn\"] > row[\"median_churn\"]:\n",
    "            strong_windows += 1\n",
    "\n",
    "    if strong_windows >= 2:\n",
    "        robust_cols.append(base)\n",
    "\n",
    "candidate_features = []\n",
    "\n",
    "for base in robust_cols:\n",
    "    for w in [\"recent\", \"mid\", \"long\"]:\n",
    "        candidate_features.append(f\"{w}_{base}\")\n",
    "\n",
    "\n",
    "final_behavior_features = [\n",
    "    \"recent_num_25\",\n",
    "    \"recent_num_50\",\n",
    "    \"recent_num_985\",\n",
    "    \"mid_num_25\",\n",
    "    \"mid_num_50\",\n",
    "    \"mid_num_985\",\n",
    "    \"mid_num_unq\",\n",
    "    \"long_num_25\",\n",
    "    \"long_num_50\",\n",
    "    \"long_num_985\",\n",
    "    \"long_num_unq\"\n",
    "]\n",
    "''',\n",
    "    \"long_total_secs\"\n",
    "'''\n",
    "\n",
    "# Week 3 Base ROC Calculation \n",
    "X_base = baseline.drop(columns=[\"msno\", \"is_churn\"])\n",
    "y = baseline[\"is_churn\"]\n",
    "\n",
    "Xb_tr, Xb_val, y_tr, y_val = train_test_split(\n",
    "    X_base, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_base = LogisticRegression(max_iter=3000)\n",
    "model_base.fit(Xb_tr, y_tr)\n",
    "\n",
    "roc_base = roc_auc_score(\n",
    "    y_val,\n",
    "    model_base.predict_proba(Xb_val)[:, 1]\n",
    ")\n",
    "\n",
    "\n",
    "# Week 4 Modelling\n",
    "X_enriched = data[\n",
    "    X_base.columns.tolist() + final_behavior_features\n",
    "]\n",
    "\n",
    "Xe_tr, Xe_val, y_tr, y_val = train_test_split(\n",
    "    X_enriched, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_enriched = LogisticRegression(max_iter=3000)\n",
    "model_enriched.fit(Xe_tr, y_tr)\n",
    "\n",
    "roc_week4 = roc_auc_score(\n",
    "    y_val,\n",
    "    model_enriched.predict_proba(Xe_val)[:, 1]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Week 3 ROC:{roc_base} & Hopefully Improved Week 4 ROC:{roc_week4}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a69352a8-e295-4a7e-80b7-0395dea5f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50777, number of negative: 743567\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5500\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Week 3 ROC:0.6928714599749448 &\n",
      "\n",
      " Linear Week 4 ROC:0.6985777033999063 &\n",
      "\n",
      " Non-Linear Week 4 ROC: 0.7428938746136711 &\n",
      "\n",
      " Importance Head:              feature  importance\n",
      "0   total_amount_paid         883\n",
      "6       recent_num_25         416\n",
      "21     recent_num_unq         415\n",
      "3     total_plan_days         353\n",
      "5          mid_num_25         347\n",
      "18     recent_num_100         333\n",
      "20        mid_num_unq         333\n",
      "19       long_num_unq         331\n",
      "9       recent_num_50         319\n",
      "4         long_num_25         317\n",
      "7         long_num_50         316\n",
      "8          mid_num_50         310\n",
      "14        mid_num_985         302\n",
      "15     recent_num_985         301\n",
      "11         mid_num_75         301\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "signal_results = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in [\"msno\", \"is_churn\"]:\n",
    "        continue\n",
    "\n",
    "    med = data.groupby(\"is_churn\")[col].median()\n",
    "\n",
    "    signal_results.append({\n",
    "        \"feature\": col,\n",
    "        \"median_churn\": med.loc[1],\n",
    "        \"median_nonchurn\": med.loc[0]\n",
    "    })\n",
    "\n",
    "signal_df = pd.DataFrame(signal_results)\n",
    "\n",
    "\n",
    "signal_df[\"base_col\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "signal_df[\"window\"] = signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "\n",
    "behavior_signal_df = signal_df[\n",
    "    signal_df[\"feature\"].str.contains(\"_\", regex=False)\n",
    "].copy()\n",
    "\n",
    "\n",
    "behavior_signal_df[\"window\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[0]\n",
    "behavior_signal_df[\"base_col\"] = behavior_signal_df[\"feature\"].str.split(\"_\", n=1).str[1]\n",
    "\n",
    "\n",
    "robust_cols = []\n",
    "\n",
    "for base in behavior_signal_df[\"base_col\"].unique():\n",
    "    temp = behavior_signal_df[\n",
    "        behavior_signal_df[\"base_col\"] == base\n",
    "    ]\n",
    "\n",
    "    strong_windows = 0\n",
    "    for _, row in temp.iterrows():\n",
    "        if row[\"median_nonchurn\"] > row[\"median_churn\"]:\n",
    "            strong_windows += 1\n",
    "\n",
    "    if strong_windows >= 2:\n",
    "        robust_cols.append(base)\n",
    "\n",
    "candidate_features = []\n",
    "\n",
    "for base in robust_cols:\n",
    "    for w in [\"recent\", \"mid\", \"long\"]:\n",
    "        candidate_features.append(f\"{w}_{base}\")\n",
    "\n",
    "\n",
    "final_behavior_features = [\n",
    "    \"recent_num_25\",\n",
    "    \"recent_num_50\",\n",
    "    \"recent_num_985\",\n",
    "    \"mid_num_25\",\n",
    "    \"mid_num_50\",\n",
    "    \"mid_num_985\",\n",
    "    \"mid_num_unq\",\n",
    "    \"long_num_25\",\n",
    "    \"long_num_50\",\n",
    "    \"long_num_985\",\n",
    "    \"long_num_unq\"\n",
    "]\n",
    "''',\n",
    "    \"long_total_secs\"\n",
    "'''\n",
    "\n",
    "# Week 3 Base ROC Calculation \n",
    "X_base = baseline.drop(columns=[\"msno\", \"is_churn\"])\n",
    "y = baseline[\"is_churn\"]\n",
    "\n",
    "Xb_tr, Xb_val, y_tr, y_val = train_test_split(\n",
    "    X_base, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_base = LogisticRegression(max_iter=3000)\n",
    "model_base.fit(Xb_tr, y_tr)\n",
    "\n",
    "roc_base = roc_auc_score(\n",
    "    y_val,\n",
    "    model_base.predict_proba(Xb_val)[:, 1]\n",
    ")\n",
    "\n",
    "\n",
    "# Week 4 Modelling with logistic regression\n",
    "X_enriched = data[\n",
    "    X_base.columns.tolist() + final_behavior_features\n",
    "]\n",
    "\n",
    "Xe_tr, Xe_val, y_tr, y_val = train_test_split(\n",
    "    X_enriched, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_enriched = LogisticRegression(max_iter=3000)\n",
    "model_enriched.fit(Xe_tr, y_tr)\n",
    "\n",
    "roc_week4 = roc_auc_score(\n",
    "    y_val,\n",
    "    model_enriched.predict_proba(Xe_val)[:, 1]\n",
    ")\n",
    "\n",
    "\n",
    "# Week 4 Modelling with non-linear dataset\n",
    "X = data.drop(columns=[\"msno\", \"is_churn\"])\n",
    "y = data[\"is_churn\"]\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = lgb_model.predict_proba(X_val)[:, 1]\n",
    "roc_lgb = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": lgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "\n",
    "print(f\"Week 3 ROC:{roc_base} &\\n\\n Linear Week 4 ROC:{roc_week4} &\\n\\n Non-Linear Week 4 ROC: {roc_lgb} &\\n\\n Importance Head:{importance.head(15)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5364e8dd-70fa-4abe-8a99-41ad8d03d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 4 final dataset saved at: F:/AI Project/churn-retention-platform/data/processed/dataset_week4_transactional_behavioral_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Just saving the merged data to a csv as concrete for future work \n",
    "import pandas as pd\n",
    "\n",
    "agg_logs = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/user_log_aggregated_week4.csv\"\n",
    ")\n",
    "\n",
    "pivoted = agg_logs.pivot(\n",
    "    index=\"msno\",\n",
    "    columns=\"win_bucket\"\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pivoted.columns = [\n",
    "    f\"{bucket}_{col}\"\n",
    "    for col, bucket in pivoted.columns\n",
    "]\n",
    "\n",
    "pivoted = pivoted.reset_index().fillna(0)\n",
    "\n",
    "\n",
    "baseline = pd.read_csv(\n",
    "    \"F:/AI Project/churn-retention-platform/data/processed/dataset_week3_pathA_final.csv\"\n",
    ")\n",
    "\n",
    "data = baseline.merge(\n",
    "    pivoted,\n",
    "    on=\"msno\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "FINAL_WEEK4_PATH = \"F:/AI Project/churn-retention-platform/data/processed/dataset_week4_transactional_behavioral_final.csv\"\n",
    "\n",
    "data.to_csv(\n",
    "    FINAL_WEEK4_PATH,\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"Week 4 final dataset saved at: {FINAL_WEEK4_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (churn-env)",
   "language": "python",
   "name": "churn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
